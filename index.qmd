---
title: "Forecasting Unemployment rate n Australia using Bayesian VARs with Regime change"
author: "Nhu Minh Pham"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** Research on how Bayesian VARs with regime change affect the forecasting 
of unemployment rate in Australia
>
> **Keywords.** bsvars, minnesota, dummy-observation, regime change, forecasting, R, 

# Introduction

The objective of this research project is to develop a Bayesian Vector Autoregression (BVAR) model incorporating regime changes to analyse the dynamic relationships among macroeconomic variables, assess their impact on the unemployment rate over different economic regimes through time-varying parameters, and enhance forecasting accuracy.

The question to be addressed in this research is: Can a Bayesian VAR model with regime changes accurately capture shifts in the relationships between unemployment rate and other macroeconomic variables for improved forecasting performance?

# Motivation
Understanding the impact of economic regime changes on the unemployment rate dynamics holds substantial importance in grasping the complexities of the labour market. The labour market is highly susceptible to various shocks and policy interventions, making accurate forecasting of the unemployment rate crucial for informed decision-making. By employing a Bayesian Vector Autoregression (BVAR) model that allows the parameters to vary across different economic regimes, we can gain a nuanced understanding of how the unemployment rate responds to diverse economic conditions and the efficacy of policies. This research enhances forecasting accuracy and offers valuable insights into the underlying drivers of unemployment fluctuations. By better understanding these dynamics, policymakers can make more informed decisions to counter economic instability.


# Data properties

For the investigation of the problem at hand, the selection of variables includes a comprehensive set of economic indicators and demographic factors that are crucial for understanding labour market dynamics and their impact on unemployment rates. 

For instance, GDP growth provides a broad measure of economic activity, serving as an indicator of overall labour market. Consumer price index, wage price index and interest rate reflect macroeconomic conditions and monetary policy, influencing consumer spending, business investment, and hiring decisions, thereby affecting unemployment trends. Government spending impacts aggregate demand and employment levels, while demographic factors such as population growth rate, average age of the workforce, and level of highest educational attainment offer insights into labour force participation and composition. For the purpose of this research, the variables are GDP, wage price index and inflation.

Each variable's form/transformation will depend on its specific characteristics and the nature of its relationship with unemployment. For instance, variables like GDP may be included in their original form, while others, such as wpi might need to be computed from the index data. 

It's important to note that all data will be quarterly since the interest forecasting for unemployment will be conducted at a quarterly frequency. Quarterly frequency is suitable for capturing the diverse movements influencing unemployment, including short-term shocks, long-term trends, and policy changes, providing a balanced perspective.



\begin{align*}
& unmp_{t} & : & \text{Unemployment rate} \\
& gdp_{t} & : & \text{GDP per capita: Chain volume measures - Percentage changes} \\
& wpi_{t} & : & \text{Wage price index} \\            
& cashrate_{t} & : & \text{Cash rate} \\
\end{align*}

```{r}
# Set global options to hide the source code, messages, and warnings
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

# Data Visualisation
```{r, hide = TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library(readrba)
library(readabs)
library(ggplot2)
library(xts)
library(tseries)
library(MSwM)
library(xtable)
library(knitr)
library(MASS) 
library(mvtnorm)
library(HDInterval)
library(plot3D)
library(Rcpp)
library(RcppArmadillo)
library(gridExtra)
library(plotly)
```



```{r,echo=FALSE, warning=FALSE, message=FALSE}
start_date <- as.Date("1991-03-01")
end_date <- as.Date("2023-12-01")

########################################
# Define a function to process ABS data
########################################
process_abs_data <- function(series_id, start_date, end_date) {
  # Read raw data from ABS
  abs_data <- read_abs(series_id = series_id)
  
  # Convert to xts object
  abs_xts <- xts(abs_data$value, order.by = as.Date(abs_data$date))
  
  # Change frequency
  abs_quarterly <- apply.quarterly(abs_xts, last)
  
  # Convert to dataframe
  abs_df <- data.frame(Date = index(abs_quarterly), Value = coredata(abs_quarterly))
  
  # Subset to specified date range
  abs_df <- abs_df[abs_df$Date >= start_date & abs_df$Date <= end_date, ]
  
  # Return the processed dataframe
  return(abs_df)
}

########################################
# Define a function to process RBA data
########################################
process_rba_data <- function(series_id, start_date, end_date) {
  # Read raw data from ABS
  rba_data <- read_rba(series_id = series_id)
  
  # Convert to xts object
  rba_xts <- xts(rba_data$value, order.by = as.Date(rba_data$date))
  
  rba_quarterly <- apply.quarterly(rba_xts, mean)
  
  # Convert to dataframe
  rba_df <- data.frame(Date = index(rba_quarterly), Value = coredata(rba_quarterly))
  
  # Subset to specified date range
  rba_df <- rba_df[rba_df$Date >= start_date & rba_df$Date <= end_date, ]
  
  # Return the processed dataframe
  return(rba_df)
}

```


```{r,echo=FALSE, warning=FALSE, message=FALSE}

#### check flow and stock, stock = end of quarter, flow= sum up

# Unemployment rate ################## "6202.0", A84423130A trend, A84423046K seasonadj, rate A84423050A
unmp_df <- process_abs_data("A84423050A", start_date, end_date)

# GDP per capita: Chain volume measures; ##### "5206.0", A2304404C seasonadj, old A2304372W
gdp_df <- process_abs_data("A2304404C", start_date, end_date)

# WPI, Total hourly rates of pay including bonuses ###### 6345.0, A2713849C seasonadj
wpi_df <- process_abs_data("A2713849C", start_date, end_date)

# Cash rate ################## original
cashrate_df <- process_rba_data("FIRMMCRT", start_date, end_date)

```


```{r, plot, echo=FALSE, fig.width=8, fig.height=6}
dataframes <- list(unmp_df = "Unemployment rate",
                    gdp_df = "GDP per capita",
                    wpi_df = "WPI",
                    cashrate_df = "Cash rate")

par(mfrow = c(2, 2))

for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  plot(df$Date, y_values, type = "l", 
       xlab = "Date", 
       ylab = "Value", 
       main = dataframes[[df_name]])
}
```
<div id="figure1" style="text-align: center; color: #696969;">
###### Figure 1: Time series plots
</div>

```{r}
# Log-transformation
log_dataframes <- c("gdp_df", "wpi_df")

# Log-transform the specified dataframes
for (df_name in log_dataframes) {
  df <- get(df_name)
  df$Value <- log(df$Value)  # Log-transform the 'Value' column
  assign(df_name, df)         # Assign the modified dataframe back to its original name
}

```

To stabilize the variance, log tranformation is performed on all variables except 
unemployment rate.

```{r, echo=FALSE, fig.width=8, fig.height=6}

dataframes <- list(unmp_df = "Unemployment rate",
                    gdp_df = "GDP per capita",
                    wpi_df = "WPI",
                    cashrate_df = "Cash rate")

par(mfrow = c(2, 2))

for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  plot(df$Date, y_values, type = "l", 
       xlab = "Date", 
       ylab = "Value", 
       main = dataframes[[df_name]])
}
```
<div id="figure1" style="text-align: center; color: #696969;">
###### Figure 1: Transformed Time series plots
</div>


From the plot of the variables, some show stationary and some non-stationary, which can create some challenges in general analysis . Therefore, the ACF test is performed to identify patterns and trend.


```{r,echo=FALSE, fig.width=8, fig.height=6}
par(mfrow = c(2, 2))

# Loop through each dataframe and create ACF plot
for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  acf(y_values, plot = TRUE, na.action = na.pass, main = paste("ACF of",
                                                               dataframes[[df_name]]))
}
```
<div id="figure2" style="text-align: center; color: #696969;">
###### Figure 2: ACF plots
</div>

The ACF plots show the variables have persistence or dependence in the data as the they are slowly decaying, indicates that there is a strong correlation between each variable 
and its past values, though this auto correlations decrease as lags increase.

```{r,echo=FALSE, fig.width=8, fig.height=6}
par(mfrow = c(2, 2))

# Loop through each dataframe and create ACF plot
for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  pacf(y_values, plot = TRUE, na.action = na.pass, main = paste("PACF of",
                                                               dataframes[[df_name]]))
}
```
<div id="figure3" style="text-align: center; color: #696969;">
###### Figure 3: PACF plots
</div>

The PACF plots show high autocorrelation at lag 1, and have clear cut offs.

The ADF is performed to further determine the stationary properties of the data.
Null hypothesis: a unit root is present
Alternative hypothesis: stationary
```{r}
# Perform ADF test for each variable
dataframes <- list(unmp_df = "Unemployment rate",
                    gdp_df = "GDP per capita",
                    wpi_df = "WPI",
                    cashrate_df = "Cash rate")

# Create an empty dataframe to store ADF test results
adf <- data.frame(Dickey_Fuller = numeric(length(dataframes)), 
                  p_value = numeric(length(dataframes)))

# Set row names to variable names
rownames(adf) <- names(dataframes)

# Perform ADF test for each variable
for (i in 1:length(dataframes)) {
  df <- get(names(dataframes)[i])  # Get the dataframe corresponding to the current variable
  adf_tmp <- adf.test(df$Value)    # Perform ADF test on the 'Value' column of the dataframe
  adf[i, "Dickey_Fuller"] <- as.numeric(adf_tmp$statistic)
  adf[i, "p_value"] <- as.numeric(adf_tmp$p.value)
}

# Round the numeric values to desired precision
adf <- round(adf, 3)

# Print the ADF test results
print(adf)
```


# Model

## Hypothesis

Var(p) model
\begin{aligned}
y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 x_{1, t-1} + \beta_3 x_{2, t-1} + \ldots + \beta_n x_{n, t-1} + \varepsilon_t
\end{aligned}


Var(p) model with regime change, meaning time-varying parameters
\begin{aligned}
y_t &= \beta_{0,S_t} + \beta_{1,S_t} y_{t-1} + \beta_{2,S_t} x_{1, t-1} + \beta_{3,S_t} x_{2, t-1} + \ldots + \beta_{n,S_t} x_{n, t-1} + \varepsilon_t \\
\end{aligned}

Probability of transitioning from one state to another
\begin{aligned}
P(S_t = j | S_{t-1} = i) &= \pi_{ij}
\end{aligned}



### Matrix form


\begin{align*}
\mathbf{Y}_t = \boldsymbol{\beta}_{S_t} \mathbf{X}_t + \boldsymbol{E}_t
\end{align*}

\begin{align*}
\boldsymbol{E}_t
\sim MN(0_T, \Sigma, I_T)
\end{align*}

\\begin{align*}
\\mathbf{Y}_t & : \\text{Matrix of response variables} \\\\
\\boldsymbol{\\beta} & : \\text{Coefficient matrix corresponding to regime state} \\\\
S_t & : \\text{Regime state} \\\\
\\mathbf{X}_t & : \\text{Matrix of predictor variables} \\\\
\\boldsymbol{\\varepsilon}_t & : \\text{Error term vector} 
\\end{align*}

\\begin{align*}
\\mathbf{Y}_t = \\begin{pmatrix}
\\text{Unemployment}_t \\\\
\\text{GDP}_t \\\\
\\text{WPI}_t \\\\
\\text{CashRate}_t \\\\
\\end{pmatrix}
\\end{align*}

The model's equations include time-varying parameters, such as the coefficients of lagged variables, which adapt to changing economic conditions.

```{r}
# Combine data frames into one data frame
# Truncate dates to the first day of the month
cashrate_df$Date <- as.Date(format(cashrate_df$Date, "%Y-%m-01"))
# Merge data frames by Date
merged_df <- merge(unmp_df, gdp_df, by = "Date", suffixes = c("_unmp", "_gdp"))
merged_df <- merge(merged_df, wpi_df, by = "Date", suffixes = c("", "_wpi"))
merged_df <- merge(merged_df, cashrate_df, by = "Date", suffixes = c("", "_cash"))

```

# Basic Model
The model follows the Normal Inverse Wishart distribution.

Likelihood function kernel
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}}exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\}
\end{align}


## Minnesota Prior distribution

The Minnesota prior is commonly used in Bayesian Vector Autoregression (BVAR) models due to its ability to impose shrinkage towards zero on the coefficients, effectively regularizing the estimation process. The Minnesota prior aligns effectively with the stylized fact of nonstationarity observed in macroeconomic variables.

\begin{align*}
p(A, \Sigma) = p(A \mid \Sigma) \cdot p(\Sigma) \\
A \mid \Sigma \sim \text{MN}_{K \times N} (\underline{A}, \Sigma, \underline{V}) \\
\Sigma \sim  \text{IW}_{N} (\underline{S}, \underline{v})\\
\end{align*}

With lags = 4 and N = 4

\begin{align*}
\underline{A} = \begin{bmatrix}
\mathbf{0}_{4 \times 1} & \mathbf{I}_{4} & \mathbf{0}_{4 \times (4-1)4}
\end{bmatrix}'
= \begin{bmatrix}
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\end{bmatrix}'
\end{align*}

The column-specific prior covariance of A
\begin{align*}
\underline{V} &= \text{diag}\left( \begin{bmatrix}
k_2 & k_1(\mathbf{p}^{-2} \otimes 1'_4)
\end{bmatrix} \right)\\
\mathbf{p} &= \begin{bmatrix}
1 & 2 & 3 & 4 \\
\end{bmatrix}
\end{align*}
\begin{align*}
& k_2 : \text{overall shrinkage for the constant term} \\
& k_1 : \text{overall shrinkage levels for autoregressive slopes} \\
\end{align*}

Prior covariance matrix
\begin{bmatrix}
k_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & k_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & k_1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & k_1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & k_1 & 0 & 0 & 0 & 0 & \cdots & 0\\ 
0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{16}      
\end{bmatrix}


```{r}
# Data configuration
N       = 4
p       = 4

y       = merged_df
y       = ts(y[,c(2,3,4,5)], start = c(1997,3), frequency = 4,
             names=c("unmp","gdp","wpi","cashrate"))
Y       = ts(y[5:nrow(y),], start = c(1998,3), frequency = 4)

X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:nrow(y)-i,])
}
```

### The joint posterior distribution
\begin{align*}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X)p(A,\Sigma) \\
&= L(A,\Sigma|Y,X)p(A|\Sigma)p(\Sigma)
\end{align*}

\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A}) \underline{V}^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\}
\end{align}

The full conditional joint posterior have the following form
\begin{align}
p(A|Y,X,\Sigma) &\sim MN_{K \times N}(\overline{A}, \Sigma,\overline{V} ) \\ 
p(\Sigma|Y,X) &\sim IW_{N}(\overline{S}, \overline{v})\\
\\
\overline{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1}\underline{A}) \\
\overline{v} &= T + \underline{v} \\ 
\overline{S} &= \underline{S}+Y'Y+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A}'
\overline{V}^{-1}\overline{A} \\
\end{align}


```{r, results='hide'}
# MLE
############################################################
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T
round(A.hat,3)
round(Sigma.hat,3)
round(cov2cor(Sigma.hat),3)

# Prior distribution
############################################################
kappa.1     = 1
kappa.2     = 100
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:5, 1:4] <- diag(1, nrow = 4)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1
```

The posterior draws are obtain with the following code
```{r, echo=TRUE, cache=TRUE}
###############################################
# Posterior draws Normal-inverse Wishart
###############################################
posterior_draws       = function (S, Y, X, A.prior, V.prior, S.prior, nu.prior){
    # Posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior -
                        t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
  
    # Posterior draws 
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
 
    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}

sample_minnesota        = posterior_draws(S=50000, Y, X, A.prior, V.prior, S.prior, nu.prior)
sample_minnesota_A      = sample_minnesota$A.posterior
sample_minnesota_Sigma  = sample_minnesota$Sigma.posterior
```

#######################################################################################
# Extension 1: Dummy observation prior
Cointegration is when the non-stationary variables have an equilibrium long term 
relationship or a common trend. The model is extended to incorporate dummy-observation-prior, aiming to account for the potential cointegration amongst the variables, capturing the underlying economic relationships and dynamics.

This approach allows us to leverage both the observed data and additional information about the structural relationships among variables, leading to more accurate parameter estimation and potentially improved forecasting performance.

Dummy observation prior also known as known as “sum-of-coefficients” proposed by Doan, Litterman, and Sims (1984) required a set of artificial observations for each variable. 

\begin{align*}
\underset{4\times 4}y^{+} &= diag(\frac{\overline{y_0}}{\mu}) \\
\underset{4 \times (1+4\times 4)}{x^{+}} &= [\underset{n \times 1}0,y^{+},...,y^{+}]
\end{align*}

To be consistent with cointegration, an additional prior proposed by Sims (1993)
is also added.

\begin{align*}
\underset{1\times 4}y^{++} &= \frac{\overline{y_0}^{'}}{\delta}, ,   \ i =1,2,3,4\\
\underset{1 \times (1+4\times 4)}{x^{++}} &= [\frac{1}{\delta},y^{++},...,y^{++}]\\
\end{align*}
\begin{align*}
& \overline{y_0} : \text{average of the first 4 lag observations for each variable}\\
& \mu: \text{controls variance }\\
& \delta: \text{controls the tightness of the prior}
\end{align*}

From the above, the following matrix form is created:

\begin{align*}
Y^{+} & = \begin{bmatrix} y^+ \\ y^{++}\end{bmatrix}', X^{+}  = \begin{bmatrix} x^+ \\ x^{++}\end{bmatrix}'\\
Y^{+} & = X^{+}A+E^{+} \\
Y^{+} & = \begin{bmatrix}
\frac{\overline{y_0}_1}{\mu} & 0 & 0 & 0 \\
0 & \frac{\overline{y_0}_2}{\mu} & 0 & 0 \\
0 & 0 & \frac{\overline{y_0}_3}{\mu} & 0 \\
0 & 0 & 0 & \frac{\overline{y_0}_4}{\mu} \\
\frac{\overline{y_0}_1}{\delta} & \frac{\overline{y_0}_2}{\delta} & \frac{\overline{y_0}_3}{\delta} & \frac{\overline{y_0}_4}{\delta} \\
\end{bmatrix}
\end{align*}

\begin{align*}
X^{+} = \begin{bmatrix}
0 & \frac{\overline{y_0}_1}{\mu} & 0 & 0 & 0 & \cdots & 0 \\
0 & 0 & \frac{\overline{y_0}_2}{\mu} & 0 & 0 & \cdots & 0 \\
0 & 0 & 0 & \frac{\overline{y_0}_3}{\mu} & 0 & \cdots & 0 \\
0 & 0 & 0 & 0 & \frac{\overline{y_0}_4}{\mu} & \cdots & \frac{\overline{y_0}_4}{\mu} \\
\frac{1}{\delta} & \frac{\overline{y_0}_1}{\delta} & \frac{\overline{y_0}_2}{\delta} & \frac{\overline{y_0}_3}{\delta} & \frac{\overline{y_0}_4}{\delta} & \cdots & \frac{\overline{y_0}_4}{\delta} \\
\end{bmatrix}
\end{align*}

The prior distribution is

\begin{align*}
A \mid \Sigma &\sim \text{MN}_{K \times N} (\underline{A}^{+}, \Sigma, \underline{V}^{+}) \\
\Sigma &\sim  \text{IW}_{N} (\underline{S}^{+}, \underline{v}^{+})\\
\\
\underline{A}^{+} &= (X^{+'}X )^{-1}X^{+'}Y^{+} \\
\underline{V}^{+} &= (X^{+'}X )^{-1} \\
\underline{v}^{+} &= T^{+} - K - N - 1\\ 
\underline{S}^{+} &= (Y^{+}-X^{+}\underline{A}^{+})^{'}(Y^{+}-X^{+}\underline{A}^{+})\\
\end{align*}


## Minnesota and dummy observation prior distribution 
The newly created matrix specified the model has 5 dummy observations. The above prior distribution is likely to encounter the problem of not having enough observations and 
lead to many challenges to meet the requirements for degree of freedom or 
invertibility of matrices. To resolve this, the combination of specifying the prior 
through both Normal inverse Wishart and dummy-observations is proposed, which is derived similarly to the posterior distribution in the basic model. 


\begin{align*}
A \mid \Sigma &\sim \text{MN}_{K \times N} (\widetilde{A}, \Sigma, \widetilde{V}) \\
\Sigma &\sim  \text{IW}_{N} (\widetilde{S}, \widetilde{v})\\
\\
\widetilde{A} &= \widetilde{V}(X^{+'}Y^{+}+\underline{V}^{-1}\underline{A}) \\
\widetilde{V} &= (X^{+'}X^{+} + \underline{V}^{-1})^{-1} \\
\widetilde{v} &= T^{+} + \underline{v} \\ 
\widetilde{S} &= \underline{S}+Y^{+'}Y^{+}+\underline{A}'\underline{V}^{-1}\underline{A} -\widetilde{A}'\widetilde{V}^{-1}\widetilde{A} \\
\end{align*}

The full conditional joint posterior have the following form
\begin{align}
p(A|Y,X,\Sigma) &\sim MN_{K \times N}(\overline{A}, \Sigma,\overline{V} ) \\ 
p(\Sigma|Y,X) &\sim IW_{N}(\overline{S}, \overline{v})\\
\\
\overline{V} &= (X'X + \widetilde{V}^{-1})^{-1} \\
\overline{A} &= \overline{V}(X'Y+\widetilde{V}^{-1}\widetilde{A}) \\
\overline{v} &= T + \widetilde{v} \\ 
\overline{S} &= \widetilde{S}+Y'Y+\widetilde{A}'\widetilde{V}^{-1}\widetilde{A}-\overline{A}'
\overline{V}^{-1}\overline{A} \\
\end{align}

The posterior draws are obtain with the following code
```{r, echo=TRUE, cache=TRUE}
###############################################
# Posterior draws for dummy prior
###############################################
posterior_draws_dummy = function(S, Y, X, A.prior, V.prior, S.prior, nu.prior) {
  
  N       = ncol(Y)
  p       = (ncol(X) - 1)/N
  mu      = 1000
  delta   = 1000
  dim(X)
  avg_unmp <- mean(X[1, c(2, 6, 10, 14)])
  avg_gdp <- mean(X[1, c(3, 7, 11, 15)])
  avg_wpi <- mean(X[1, c(4, 8, 12, 16)])
  avg_cash <- mean(X[1, c(5, 9, 13, 17)])
  
  diag_mu <- diag(c(avg_unmp/mu, avg_gdp/mu, avg_wpi/mu, avg_cash/mu))
  row_delta <-  matrix(c(avg_unmp/delta, avg_gdp/delta, avg_wpi/delta, avg_cash/delta), nrow = 1)
  
  Y_d       = rbind(diag_mu, row_delta)
  
  first_vector <- matrix(c(0,0,0,0, 1/delta), ncol = 1)
  X_d      = cbind(first_vector, Y_d, Y_d, Y_d, Y_d)
  
  # Normal-inverse Wishart combination of specifying the prior 
  ############################################################
  V.tilde.inv   = t(X_d)%*%X_d + diag(1/diag(V.prior))
  V.tilde       = solve(V.tilde.inv)
  A.tilde       = V.tilde%*%(t(X_d)%*%Y_d + diag(1/diag(V.prior))%*%A.prior)
  nu.tilde      = nrow(Y_d) + nu.prior
  S.tilde       = S.prior + t(Y_d)%*%Y_d + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior -
    t(A.tilde)%*%V.tilde.inv%*%A.tilde
  S.tilde.inv   = solve(S.tilde)

  # Normal-inverse Wishart posterior parameters
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.tilde))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.tilde))%*%A.tilde)
  nu.bar      = nrow(Y) + nu.tilde
  S.bar       = S.tilde + t(Y)%*%Y + t(A.tilde)%*%diag(1/diag(V.tilde))%*%A.tilde -
                t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  

  # Posterior draws 
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
  output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
  return(output)
}
sample_dummy        = posterior_draws_dummy(S = 50000, Y, X, A.prior, V.prior, S.prior, nu.prior)
sample_dummy_A      = sample_dummy$A.posterior
sample_dummy_Sigma  = sample_dummy$Sigma.posterior
```

## Extension 2 - Regime change
With 2 regime states, we would have $Y_m$ and $X_m$

The full conditional joint posterior have the following form
\begin{align}
p(A|Y,X,\Sigma) &\sim MN_{K \times N}(\overline{A}, \Sigma,\overline{V} ) \\ 
p(\Sigma|Y,X) &\sim IW_{N}(\overline{S}, \overline{v})\\
\\
\overline{V_m} &= (X'X + \underline{V}^{-1})^{-1} \\
\overline{A_m} &= \overline{V_m}(X_m'Y_m+\underline{V}^{-1}\underline{A}) \\
\overline{v} &= T_m + \underline{v} \\ 
\overline{S_m} &= \underline{S}+Y_m'Y_m+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A_m}'
\overline{V_m}^{-1}\overline{A_m} \\
\end{align}

Markov Switching 
Initial probability of regimes
\begin{align}
\begin{bmatrix}
p_{1} & p_{2}\\
\end{bmatrix}
\end{align}

Probability transition matrix
\begin{align}
\begin{bmatrix}
p_{11} & p_{12}\\
p_{21} & p_{22}\\
\end{bmatrix}
\end{align}

\begin{align}
\xi_{t} = \begin{bmatrix}
1\\
0\\
\end{bmatrix}, if s_t = 1
\end{align}

\begin{align}
\xi_{t} = \begin{bmatrix}
0\\
1\\
\end{bmatrix}, if s_t = 2
\end{align}
The posterior draws are done with Gibb sampling. 

```{r}
# Data configuration
N       = 4
p       = 4

y       = merged_df
y       = ts(y[,c(2,3,4,5)], start = c(1997,3), frequency = 4,
              names=c("unmp","gdp","wpi","cashrate"))
Y       = ts(y[5:nrow(y),], start = c(1998,3), frequency = 4)

X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:nrow(y)-i,])
}

A.hat = solve(t(X) %*% X) %*% t(X) %*% Y
Sigma.hat = t(Y - X %*% A.hat) %*% (Y - X %*% A.hat) / T

kappa.1 = 1
kappa.2 = 100
A.prior = matrix(0, nrow = nrow(A.hat), ncol = ncol(A.hat))
A.prior[2:5, 1:4] = diag(1, nrow = 4)
V.prior = diag(c(kappa.2, kappa.1 * ((1:p)^(-2)) %x% rep(1, N)))
S.prior = diag(diag(Sigma.hat))
nu.prior = N + 1
```

```{r, echo=FALSE, results ='hide'}
#Rcpp::sourceCpp ("ms.cpp")
invisible({
  # Redirect plot output to a temporary file
  tmp_file <- tempfile(fileext = ".pdf")
  pdf(file = tmp_file)
  Rcpp::sourceCpp("ms.cpp")
  dev.off()
  unlink(tmp_file)  # Remove the temporary file
})  
```

```{r}
# Gibbs sampling function
Gibbs_draws = function(S, Y, X, xi, P, pi) {
  N = ncol(Y)
  p = (ncol(X) - 1) / N 
  M = 2   # Number of regimes
  T = nrow(Y)
  
  Sigma.posterior.draws1 = array(NA, c(N, N, S))
  Sigma.posterior.draws2 = array(NA, c(N, N, S))
  A.posterior.draws1 = array(NA, c((1 + p * N), N, S))
  A.posterior.draws2 = array(NA, c((1 + p * N), N, S))
  xi.draws <- array(NA, dim = c(M, T, S))
  P.draws <- array(NA, c(2, 2, S))
  pi.draws <- matrix(NA, nrow = M, ncol = S)

  P = 0.95 * diag(2) + 0.025
  pi = c(0.5, 0.5)
  xi = rbind(c(rep(1,50), rep(0,T-50)), c(rep(0,50), rep(1,T-50)))
  Z = array(rnorm(T*N*M), c(N,T,M))

  
  for (s in 1:S) {
    # Update xi, P, and pi using the sample_Markov_process function
    gibbs_draw = sample_Markov_process(Z, xi, P, pi)
    xi = gibbs_draw$aux_xi
    P = gibbs_draw$aux_PR_TR
    pi = gibbs_draw$aux_pi_0
    
    xi.draws[, , s] <- xi
    P.draws[, , s] <- P
    pi.draws[, s] <- pi
    
    # Loop to get regime-specific Y, X, Sigma, A
    for (m in 1:2) {
      Y_m =  Y[xi[m,]==1,]
      X_m =  X[xi[m,]==1,]
      T_m = nrow(Y_m)
      
      if (T_m > 0) {
        # Posterior parameters
        V.bar.inv = t(X_m) %*% X_m + diag(1 / diag(V.prior))
        V.bar = solve(V.bar.inv)
        A.bar = V.bar %*% (t(X_m) %*% Y_m + diag(1 / diag(V.prior)) %*% A.prior)
        nu.bar = T_m + nu.prior
        S.bar = S.prior + t(Y_m) %*% Y_m + t(A.prior) %*% diag(1 / diag(V.prior)) %*% A.prior - t(A.bar) %*% V.bar.inv %*% A.bar
        S.bar.inv = solve(S.bar)
        
        # Posterior draws for Sigma
        Sigma.posterior = rWishart(1, df = nu.bar, Sigma = S.bar.inv)
        Sigma.posterior = solve(Sigma.posterior[, , 1])
        if (m == 1) {
          Sigma.posterior.draws1[, , s] = Sigma.posterior
        } else {
          Sigma.posterior.draws2[, , s] = Sigma.posterior
        }
        
        # Posterior draws for A
        L = t(chol(V.bar))
        xx = matrix(rnorm(prod(dim(A.bar))), dim(A.bar)[1], dim(A.bar)[2])
        
        A.posterior <- A.bar + L %*% xx %*% chol(Sigma.posterior)
        if (m == 1) {
          A.posterior.draws1[, , s] = A.posterior
        } else {
          A.posterior.draws2[, , s] = A.posterior
        }
        
        # Compute Z_m for the next iteration
        LL = t(chol(Sigma.posterior))
        Z[, , m] <-   t((Y - X %*% A.posterior) %*% LL)
      }
    }
  }

  output = list(
    A.posterior1 = A.posterior.draws1,
    A.posterior2 = A.posterior.draws2,
    Sigma.posterior1 = Sigma.posterior.draws1,
    Sigma.posterior2 = Sigma.posterior.draws2,
    xi = xi.draws,
    P = P.draws,
    pi = pi.draws
  )
  return(output)
}

label_switching <- function(draws) {
  S <- dim(draws$Sigma.posterior1)[3]  # Number of draws
  
  for (s in 1:S) {
    if (draws$Sigma.posterior1[1, 1, s] < draws$Sigma.posterior2[1, 1, s]) {
      # Swap Sigma
      temp_Sigma <- draws$Sigma.posterior1[,,s]
      draws$Sigma.posterior1[,,s] <- draws$Sigma.posterior2[,,s]
      draws$Sigma.posterior2[,,s] <- temp_Sigma
      
      # Swap A
      temp_A <- draws$A.posterior1[,,s]
      draws$A.posterior1[,,s] <- draws$A.posterior2[,,s]
      draws$A.posterior2[,,s] <- temp_A
      
      # Swap rows of xi
      temp_xi_row1 <- draws$xi[1,,s]
      draws$xi[1,,s] <- draws$xi[2,,s]
      draws$xi[2,,s] <- temp_xi_row1
      
      # Swap rows and columns of P
      draws$P[,,s] <- draws$P[c(2,1), c(2,1), s]
      
      # Swap columns of pi
      draws$pi[, s] <- draws$pi[c(2, 1), s]
    }
  }
  
  return(draws)
}
```


```{r}
Markov.draws = Gibbs_draws(S = 50000, Y, X, xi, P, pi)
before <- ts(t(apply(Markov.draws$xi, 1:2, mean)), start = c(1998,3), frequency = 4)
```

To examine how the regime draws are distributed between the two states, we visualize the switching variable $\xi$. Initially, the plot suggests that the regime assignments appear arbitrary. To address this issue, a label switching procedure is implemented to ensure consitency in how the two regimes are identified, in this case regime 1 is identified as the state with higher volatility. Following the label switching, the distinction between the two regimes becomes more evident. The drawn regimes indicate that regime 1 corresponds to the period around 2020, which aligns with the Covid-19 pandemic. This finding is intuitive as the Covid-19 pandemic significantly impacted the economy at large, including unemployment rates. Regime 2, on the other hand, represents other periods.


```{r, fig.width=8, fig.height=6}
Markov.draws <- label_switching(Markov.draws)

after <- ts(t(apply(Markov.draws$xi, 1:2, mean)), start = c(1998,3), frequency = 4)
```

```{r, fig.width=8, fig.height=6 }
plots <- list(before, after)
titles <- c("Before label switching", "After label switching")

par(mfrow = c(1, 2))
for (i in seq_along(plots)) {
  plot.ts(plots[[i]], main = titles[i])
}
```

## Final model
The final model is the combination of Minnesota prior, dummy observation prior and 
regime change to account for heteroskedasticity.

```{r}
# Data configuration
N       = 4
p       = 4

y       = merged_df
y       = ts(y[,c(2,3,4,5)], start = c(1997,3), frequency = 4,
              names=c("unmp","gdp","wpi","cashrate"))
Y       = ts(y[5:nrow(y),], start = c(1998,3), frequency = 4)

X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:nrow(y)-i,])
}

A.hat = solve(t(X) %*% X) %*% t(X) %*% Y
Sigma.hat = t(Y - X %*% A.hat) %*% (Y - X %*% A.hat) / T

kappa.1 = 1
kappa.2 = 100
A.prior = matrix(0, nrow = nrow(A.hat), ncol = ncol(A.hat))
A.prior[2:5, 1:4] = diag(1, nrow = 4)
V.prior = diag(c(kappa.2, kappa.1 * ((1:p)^(-2)) %x% rep(1, N)))
S.prior = diag(diag(Sigma.hat))
nu.prior = N + 1
```

```{r}
# Gibbs sampling function
FinalGibbs_draws = function(S, Y, X, xi, P, pi, A.prior, V.prior, S.prior, nu.prior) {
  N = ncol(Y)
  p = (ncol(X) - 1) / N 
  M = 2   # Number of regimes
  T = nrow(Y)
  
  mu      = 1000
  delta   = 1000
  
  avg_unmp <- mean(X[1, c(2, 6, 10, 14)])
  avg_gdp <- mean(X[1, c(3, 7, 11, 15)])
  avg_wpi <- mean(X[1, c(4, 8, 12, 16)])
  avg_cash <- mean(X[1, c(5, 9, 13, 17)])
  
  diag_mu <- diag(c(avg_unmp/mu, avg_gdp/mu, avg_wpi/mu, avg_cash/mu))
  row_delta <-  matrix(c(avg_unmp/delta, avg_gdp/delta, avg_wpi/delta, avg_cash/delta), 
                       nrow = 1)
  
  Y_d       = rbind(diag_mu, row_delta)
  
  first_vector <- matrix(c(0,0,0,0, 1/delta), ncol = 1)
  X_d      = cbind(first_vector, Y_d, Y_d, Y_d, Y_d)
  
  # Normal-inverse Wishart prior parameters
  ############################################################
  V.tilde.inv   = t(X_d)%*%X_d + diag(1/diag(V.prior))
  V.tilde       = solve(V.tilde.inv)
  A.tilde       = V.tilde%*%(t(X_d)%*%Y_d + diag(1/diag(V.prior))%*%A.prior)
  nu.tilde      = nrow(Y_d) + nu.prior
  S.tilde       = S.prior + t(Y_d)%*%Y_d + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior -
    t(A.tilde)%*%V.tilde.inv%*%A.tilde
  S.tilde.inv   = solve(S.tilde)

  Sigma.posterior.draws1 = array(NA, c(N, N, S))
  Sigma.posterior.draws2 = array(NA, c(N, N, S))
  A.posterior.draws1 = array(NA, c((1 + p * N), N, S))
  A.posterior.draws2 = array(NA, c((1 + p * N), N, S))
  xi.draws <- array(NA, dim = c(M, T, S))
  P.draws <- array(NA, c(2, 2, S))
  pi.draws <- matrix(NA, nrow = M, ncol = S)

  P = 0.95 * diag(2) + 0.025
  pi = c(0.5, 0.5)
  xi = rbind(c(rep(1,50), rep(0,T-50)), c(rep(0,50), rep(1,T-50)))
  Z = array(rnorm(T*N*M), c(N,T,M))

  
  for (s in 1:S) {
    # Update xi, P, and pi using the sample_Markov_process function
    gibbs_draw = sample_Markov_process(Z, xi, P, pi)
    xi = gibbs_draw$aux_xi
    P = gibbs_draw$aux_PR_TR
    pi = gibbs_draw$aux_pi_0
    
    xi.draws[, , s] <- xi
    P.draws[, , s] <- P
    pi.draws[, s] <- pi
    
    # Loop to get regime-specific Y, X, Sigma, A
    for (m in 1:2) {
      Y_m =  Y[xi[m,]==1,]
      X_m =  X[xi[m,]==1,]
      T_m = nrow(Y_m)
      
      if (T_m > 0) {
        # Posterior parameters
        V.bar.inv = t(X_m) %*% X_m + diag(1 / diag(V.tilde))
        V.bar = solve(V.bar.inv)
        A.bar = V.bar %*% (t(X_m) %*% Y_m + diag(1 / diag(V.tilde)) %*% A.tilde)
        nu.bar = T_m + nu.tilde
        S.bar = S.tilde + t(Y_m) %*% Y_m + t(A.tilde) %*% diag(1 / diag(V.tilde)) %*% A.tilde -                 t(A.bar) %*% V.bar.inv %*% A.bar
        S.bar.inv = solve(S.bar)
        
        # Posterior draws for Sigma
        Sigma.posterior = rWishart(1, df = nu.bar, Sigma = S.bar.inv)
        Sigma.posterior = solve(Sigma.posterior[, , 1])
        if (m == 1) {
          Sigma.posterior.draws1[, , s] = Sigma.posterior
        } else {
          Sigma.posterior.draws2[, , s] = Sigma.posterior
        }
        
        # Posterior draws for A
        L = t(chol(V.bar))
        xx = matrix(rnorm(prod(dim(A.bar))), dim(A.bar)[1], dim(A.bar)[2])
        
        A.posterior <- A.bar + L %*% xx %*% chol(Sigma.posterior)
        if (m == 1) {
          A.posterior.draws1[, , s] = A.posterior
        } else {
          A.posterior.draws2[, , s] = A.posterior
        }
        
        # Compute Z_m for the next iteration
        LL = t(chol(Sigma.posterior))
        Z[, , m] <-   t((Y - X %*% A.posterior) %*% LL)
      }
    }
  }

  output = list(
    A.posterior1 = A.posterior.draws1,
    A.posterior2 = A.posterior.draws2,
    Sigma.posterior1 = Sigma.posterior.draws1,
    Sigma.posterior2 = Sigma.posterior.draws2,
    xi = xi.draws,
    P = P.draws,
    pi = pi.draws
  )
  return(output)
}

Final.draws = FinalGibbs_draws(S = 50000, Y, X, xi, P, pi, A.prior, V.prior, S.prior, nu.prior)
before_f <- ts(t(apply(Final.draws$xi, 1:2, mean)), start = c(1998,3), frequency = 4)
```

```{r}
Final.draws <- label_switching(Final.draws)
after_f <- ts(t(apply(Final.draws$xi, 1:2, mean)), start = c(1998,3), frequency = 4)
```

```{r, fig.width=8, fig.height=6}
par(mfrow = c(1, 2))
plot.ts(before_f, main = "Before label switching")
plot.ts(after_f, main = "After label switching")
```

```{r, fig.width=8, fig.height=6}
par(mfrow = c(1, 2))
plot(before_f, type = "l", main = "Before label switching", xlab = "Date", ylab = "Value")
plot(after_f, type = "l", main = "Before label switching", xlab = "Date", ylab = "Value")
```

# Forecasting
## Basic model
```{r}
posterior_summary <- function(sample) {
  # Report posterior means and sd of parameters
  A.E <- round(apply(sample$A.posterior, 1:2, mean), 6)
  A.sd <- round(apply(sample$A.posterior, 1:2, sd), 6)
  Sigma.E <- round(apply(sample$Sigma.posterior, 1:2, mean), 6)
  Sigma.sd <- round(apply(sample$Sigma.posterior, 1:2, sd), 6)
  
  # Create tables
  A_table <- data.frame(rbind(t(A.E)[1,], t(A.sd)[1,], t(A.E)[2,], t(A.sd)[2,]))
  Sigma_table <- data.frame(rbind(t(Sigma.E)[1,], t(Sigma.sd)[1,], t(Sigma.E)[2,],
                                  t(Sigma.sd)[2,]))
  # Format tables using kable
  A_table_html <- kable(A_table, format = "html", caption = "Posterior A means and s.d. deviation table",
                        col.names = paste0(1:ncol(A_table)))
  Sigma_table_html <- kable(Sigma_table, format = "html", caption = "Posterior Sigma means and s.d. table", col.names = paste0(1:ncol(Sigma_table)))
  html_content <- paste(A_table_html, Sigma_table_html, sep = "\n\n")
  
  # Return the HTML content
  return(html_content)
  # Return tables as list
  #return(A_table_html, Sigma_table_html)
  #return(list(A_table_html = A_table_html, Sigma_table_html = Sigma_table_html))
}
  

############################################################
# Point forecasts 2D plot
forecast_plots <- function(Y, Y.h, h, theta, phi, xlim) {
  
  # Define colors
  mcxs1  = "#05386B"
  mcxs2  = "#379683"
  mcxs3  = "#5CDB95"
  mcxs4  = "#8EE4AF"
  mcxs5  = "#EDF5E1"
  
  mcxs1.rgb   = col2rgb(mcxs1)
  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=50, maxColorValue=255)
  mcxs2.rgb   = col2rgb(mcxs2)
  mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=50, maxColorValue=255)
  
  point_forecast    = apply(Y.h[,1,],1,mean) 
  interval_forecast = apply(Y.h[,1,],1,hdi,credMass=0.90)
  interval_forecast_1 = apply(Y.h[,1,],1,hdi,credMass=0.68)
  range             = range(Y[,1],interval_forecast)
  
  par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1)
  plot(1:(length(Y[,1])), Y[,1], type="l", axes=FALSE, 
       xlim = c(1, nrow(Y)+h), ylim=range, 
       xlab="", ylab="", 
       lwd=2, col = mcxs1, main = "Unemployment rate forecasts")
  lines(length(Y[,1]):(length(Y[,1]) + h), c(Y[nrow(Y), 1],point_forecast), type="l", 
        xlim = c(1, nrow(Y)+h), ylim=range,
        xlab="", ylab="", 
        lwd=2, col = mcxs2)
  axis(1,c(3,23,43,63,83,101,nrow(Y), nrow(Y)+h),
       c("1999","2003","2008","20013","2018","2023","", "2028"), 
       col="black")
  axis(2, at=seq(from=range[1], to=range[2], length.out=5), 
       labels=round(seq(from=range[1], to=range[2], length.out=5), 2),
       col="black")
  end_y_value <- point_forecast[13] 
  abline(h = end_y_value, col="red", lty=2, lwd=1)
  abline(v = nrow(Y), col = "black")
  legend("bottomleft", legend=c("Data", "Forecast"), 
         col=c(mcxs1, mcxs2), lty=c(1, 1), lwd=c(2, 2))
  polygon(c(length(Y[,1]):(length(Y[,1]) + h), (length(Y[,1]):(length(Y[,1]) + h))[(h+1):1]), 
          c(Y[nrow(Y), 1], interval_forecast[1,], interval_forecast[2,h:1], Y[nrow(Y), 1]), 
          col = mcxs1.shade1, border = mcxs1.shade1)
  polygon(c(length(Y[,1]):(length(Y[,1]) + h), (length(Y[,1]):(length(Y[,1]) + h))[(h+1):1]), 
          c(Y[nrow(Y), 1], interval_forecast_1[1,], interval_forecast_1[2,h:1], Y[nrow(Y), 1]), 
          col = mcxs1.shade1, border = mcxs1.shade1)
  
  limits.1    = range(Y.h[,1,])
  point.f     = apply(Y.h[,1,],1,mean)
  interval.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)
  
  x           = seq(from=limits.1[1], to=limits.1[2], length.out=100)
  z           = matrix(NA,h,99)
  for (i in 1:h){
    z[i,]     = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
  }
  x           = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
  yy          = 1:h
  z           = t(z)
  
  # Adjust xlim, ylim, zlim based on h
  xlim <- c(limits.1[1], limits.1[2])
  ylim <- c(1, h)
  zlim <- range(z)
  
  f4 <- persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, xlab="\nunmp[t+h|t]", ylab="h",
                zlab="\npredictive densities of unemployment rate", shade=NA, border=NA,
                ticktype="detailed", nticks=3,cex.lab=1, col=NA, plot=FALSE, 
                xlim = xlim, ylim = ylim, zlim = zlim)
  
  perspbox(x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta,
           xlab="\nunemployment[t+h|t]", ylab="h", zlab="\npredictive densities of unemployment rate",
           ticktype="detailed", nticks=3, cex.lab=1, col = NULL, plot = TRUE, 
           xlim = xlim, ylim = ylim, zlim = zlim)
  
  polygon3D(x = c(interval.f[1,], interval.f[2,h:1]), y = c(1:h, h:1), z = rep(0, 2*h), 
            col = mcxs1.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE, 
            xlim = xlim, ylim = ylim, zlim = zlim)
  
  for (i in 1:h){
    f4.l <- trans3d(x = x, y = yy[i], z = z[,i], pmat = f4)
    lines(f4.l, lwd = 0.5, col = "black")
  }
  
  f4.l1 <- trans3d(x = point.f, y = yy, z = 0, pmat = f4)
  lines(f4.l1, lwd = 2, col = mcxs1)
}
```


The forecasting process can be done with the following code. 
```{r, echo=TRUE}
forecast <- function(S, sample_A, sample_Sigma, Y, p, h) {
  # Initialize the array to store forecasts
  Y.h <- array(NA, c(h, N, S))
  
  # Loop over each sample
  for (s in 1:S) {
    A.posterior.draw <- sample_A[,,s]
    Sigma.posterior <- sample_Sigma[,,s]
    x.Ti <- Y[(nrow(Y) - p + 1):nrow(Y),]
    x.Ti <- x.Ti[p:1,]
    
    # Forecast h steps ahead
    for (i in 1:h) {
      x.T <- c(1, as.vector(t(x.Ti)))
      Y.f <- rmvnorm(1, mean = x.T %*% A.posterior.draw, sigma = Sigma.posterior)
      x.Ti <- rbind(Y.f, x.Ti[1:(p - 1),])
      Y.h[i,,s] <- Y.f[1:2]
    }
  }
  
  return(Y.h)
}
```


```{r, eval=FALSE}
# Report posterior means and sd of parameters
A.E         = round(apply(sample_minnesota$A.posterior,1:2,mean),6)
A.sd        = round(apply(sample_minnesota$A.posterior,1:2,sd),6)
Sigma.E     = round(apply(sample_minnesota$Sigma.posterior,1:2,mean),6)
Sigma.sd    = round(apply(sample_minnesota$Sigma.posterior,1:2,sd),6)

A_table <- data.frame(rbind(t(A.E)[1,],t(A.sd)[1,],t(A.E)[2,],t(A.sd)[2,]))
Sigma_table <- data.frame(rbind(t(Sigma.E)[1,],t(Sigma.sd)[1,],t(Sigma.E)[2,],t(Sigma.sd)[2,]))

kable(A_table, format = "html", caption = "Posterior A means and sd table",
      col.names = paste0(1:ncol(A_table)))

kable(Sigma_table, format = "html", caption = "Posterior Sigma means and sd table",
      col.names = paste0(1:ncol(Sigma_table)))


###############################################
# Report posterior means and sd of parameters
A.E         = round(apply(sample_dummy$A.posterior,1:2,mean),6)
A.sd        = round(apply(sample_dummy$A.posterior,1:2,sd),6)
Sigma.E     = round(apply(sample_dummy$Sigma.posterior,1:2,mean),6)
Sigma.sd    = round(apply(sample_dummy$Sigma.posterior,1:2,sd),6)

A_table <- data.frame(rbind(t(A.E)[1,],t(A.sd)[1,],t(A.E)[2,],t(A.sd)[2,]))
Sigma_table <- data.frame(rbind(t(Sigma.E)[1,],t(Sigma.sd)[1,],t(Sigma.E)[2,],t(Sigma.sd)[2,]))

kable(A_table, format = "html", caption = "Posterior A means and sd table",
      col.names = paste0(1:ncol(A_table)))

kable(Sigma_table, format = "html", caption = "Posterior Sigma means and sd table",
      col.names = paste0(1:ncol(Sigma_table)))
```


```{r, cache=TRUE}
N       = 4
p       = 4
S       = 50000 

posterior_summary(sample_minnesota)

Y.h_minnesota <- forecast(S, sample_minnesota_A, sample_minnesota_Sigma, Y, p, h = 21)

forecast_plots(Y, Y.h_minnesota, h = 21, theta = 190, phi = 10, xlim = c(-50, 50))
```
The plot of point forecast and density for 21 periods ahead, equivalent to 21 quarters or 5 years and 1 quarter from Q4 2023 to Q4 2028 shows a clear upward trend. The confidence interval is quite large. The unemployment rate seems to converge to the mean and has a slight downward trend in the last horizons. 

For the 3D plot of the density, we can see that the further the period ahead, the density is more dispersed since the data is become less informative as horizon increases.


## Extension 1
```{r, cache=TRUE}
N       = 4
p       = 4

# posterior_summary(sample_dummy)

Y.h_dummy <- forecast(S = 50000, sample_dummy_A,sample_dummy_Sigma, Y, p, h = 21)

forecast_plots(Y, Y.h_dummy, h =21, theta = 190, phi = 10, xlim = c(-30, 30))
```
The plot of forecast for the dummy observation prior doesn't show any significant differences visually. The confidence interval narrowed insignificantly.

## Extension 2

After the regimes are correctly placed, we can forecast h-step ahead using the posterior draws.
```{r, echo=TRUE}
Markov_forecast <- function(S, sample, Y, p, h) {
  N <- ncol(Y)
  # Initialize the array to store forecasts
  Y.h = array(NA, c(h, 2, S))

  for (s in 1:S){
    A.posterior.draw1 = sample$A.posterior1[,,s]
    A.posterior.draw2 = sample$A.posterior2[,,s]
    Sigma.posterior1 = sample$Sigma.posterior1[,,s]
    Sigma.posterior2 = sample$Sigma.posterior2[,,s]
  
    T = dim(sample$xi)[2]
  
    x.Ti = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti = x.Ti[p:1,]
    
    P_h = sample$P[,,s] %*% sample$xi[,T,s]  # Initializing P_h
  
    for (i in 1:h){
      x.T = c(1, as.vector(t(x.Ti)))
    
      if (i > 1) {  # Only update P_h if i > 1
        P_h = sample$P[,,s] %*% P_h
      }
      sample_m = sample.int(2,1, prob=as.numeric(P_h))
    
      if (sample_m == 1) {
        A.posterior.draw = A.posterior.draw1
        Sigma.posterior = Sigma.posterior1
      } else {
        A.posterior.draw = A.posterior.draw2
        Sigma.posterior = Sigma.posterior2
      }
    
      Y.f = rmvnorm(1, mean = x.T %*% A.posterior.draw, sigma = Sigma.posterior)
      x.Ti = rbind(Y.f, x.Ti[1:(p-1),])
      Y.h[i,,s] = Y.f[1:2]
    }
  }
  return(Y.h)
}
```

The plot below shows the forecast using Minnesota prior and regime change for unemployment rate.
```{r }
Y.h_Markov = Markov_forecast(S = 50000, sample = Markov.draws, Y, p, h = 21)

forecast_plots(Y, Y.h_Markov, h = 21, theta = 190, phi = 10, xlim = c(-50, 50))
```

## Final model

Similarly to the baseline model, the prior will be a combination of Minnesota and dummy observation prior. Then the Markov-switching Gibb-sampling is done with these new priors to obtain posterior draws


```{r}
Y.h_Final = Markov_forecast(S = 50000, sample = Final.draws, Y, p, h = 21)
forecast_plots(Y, Y.h_Final, h = 21, theta = 190, phi = 10, xlim = c(-10, 20))
```


```{r}
plots_3d <- function(Y.h, h, xlim) {
  mcxs1  = "#05386B"
  mcxs4  = "#8EE4AF"
  mcxs5  = "#EDF5E1"
  
  par(mfrow=c(1,2), mar=c(2,2,1,1))
  
  # Forecasting on basic model
  limits.1    <- range(Y.h[,1,])
  point.f     <- apply(Y.h[,1,], 1, mean)
  interval.f  <- apply(Y.h[,1,], 1, hdi, credMass=0.90)
  theta       <- 180
  phi         <- 15.5
  
  x           <- seq(from=limits.1[1], to=limits.1[2], length.out=10)
  z           <- matrix(NA, h, 9)
  for (i in 1:h){
    z[i,]     <- hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
  }
  x           <- hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
  yy          <- 1:h
  
  # Plot using plot_ly
  plot_ly(y = yy, x = x, z = z) %>%
    layout(scene = list(xaxis = list(title = "Unemployment rate"),
                        yaxis = list(title = "h-step forecast"),
                        zaxis = list(title = "density")),
           title = "Unemployment rate forecast densities for basic model") %>%
    add_surface(colors = c(mcxs1, mcxs4, mcxs5))
  
  # Forecasting on extended model
  limits.2    <- range(Y.h[,2,])
  point.f     <- apply(Y.h[,2,], 1, mean)
  interval.f  <- apply(Y.h[,2,], 1, hdi, credMass=0.90)
  theta       <- 180
  phi         <- 15.5
  
  x           <- seq(from=limits.2[1], to=limits.2[2], length.out=10)
  z           <- matrix(NA, h, 9)
  for (i in 1:h){
    z[i,]     <- hist(Y.h[i,2,], breaks=x, plot=FALSE)$density
  }
  x           <- hist(Y.h[i,2,], breaks=x, plot=FALSE)$mids
  yy          <- 1:h
  
  # Plot using plot_ly
  plot_ly(y = yy, x = x, z = z) %>%
    layout(scene = list(xaxis = list(title = "Unemployment rate"),
                        yaxis = list(title = "h-step forecast"),
                        zaxis = list(title = "density")),
           title = "AUD/USD forecast densities for extended model") %>%
    add_surface(colors = c(mcxs1, "magenta2", mcxs5))
}

# Example usage
plots_3d(Y.h_Final, h = 21)

```

The data used is up to September 2023. The subsequent unemployment rate is released by the 
```{r}
# Create the data frame for the table
data <- data.frame(
  Month = c("Oct-23", "Nov-23", "Dec-23", "Jan-24", "Feb-24", "Mar-24", "Apr-24"),
  ABS = c(3.8, 3.9, 4.0, 4.1, 3.7, 3.9, 4.1)
)

# Create the forecast data frame
forecast <- data.frame(
  Quarter = c("Q4-23", "Q1-24", "Q2-24"),
  Forecast = c(3.694583, 3.868261, 4.063661)
)

kable(data, caption = "ABS Data", format="html")
kable(forecast, caption = "Forecast Data", format="html") 
```



# Model Proof
1000 observations are simulated from a bi-variate Gaussian random walk process annd is used as data for the below proofs.

### Basic model
```{r, echo=FALSE}
set.seed(123456)
N       = 2
p       = 1
S       = 1000
K       = 1+p*N

r1 = cumsum(rnorm(1000,0,1))
r2 = cumsum(rnorm(1000,0,1))

r = cbind(r1, r2) 

Y_p = ts(r[2:nrow(r),], frequency=1)

X_p     = matrix(1,nrow(Y_p),1)
X_p     = cbind(X_p,r[2:nrow(r)-1,])

# MLE
############################################################
A.hat       = solve(t(X_p)%*%X_p)%*%t(X_p)%*%Y_p
Sigma.hat   = t(Y_p-X_p%*%A.hat)%*%(Y_p-X_p%*%A.hat)/nrow(Y_p)

# Prior distribution
############################################################
kappa.1     = 1
kappa.2     = 100
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

############################################################
sample_minnesota_proof = posterior_draws(1000, Y_p, X_p, A.prior, V.prior, S.prior, nu.prior)

# Report posterior means and sd of parameters
A.E         = round(apply(sample_minnesota_proof$A.posterior,1:2,mean),6)
A.sd        = round(apply(sample_minnesota_proof$A.posterior,1:2,sd),6)
Sigma.E     = round(apply(sample_minnesota_proof$Sigma.posterior,1:2,mean),6)
Sigma.sd    = round(apply(sample_minnesota_proof$Sigma.posterior,1:2,sd),6)


kable(A.E, format = "html", caption = "Posterior mean of autoregressive",
      col.names = paste0(1:ncol(A.E)))

kable(Sigma.E, format = "html", caption = "Posterior mean of covarince",
      col.names = paste0(1:ncol(Sigma.E)))

```
### Extended model
```{r, echo = FALSE}

mu      = 1000
delta   = 1000
diag_mu <- diag(c(X_p[1,2]/mu, X_p[1,3]/mu))
row_delta <-  matrix(c(X_p[1,2]/delta, X_p[1,3]/delta), nrow = 1)

Y_d       = rbind(diag_mu, row_delta)
  
first_vector <- matrix(c(0,0, 1/delta), ncol = 1)
X_d      = cbind(first_vector, Y_d)


# Normal-inverse Wishart prior parameters
############################################################
V.tilde.inv     = t(X_d)%*%X_d + diag(1/diag(V.prior))
V.tilde       = solve(V.tilde.inv)
A.tilde       = V.tilde%*%(t(X_d)%*%Y_d + diag(1/diag(V.prior))%*%A.prior)
nu.tilde      = nrow(Y_d) + nu.prior
S.tilde       = S.prior + t(Y_d)%*%Y_d + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.tilde)%*%V.tilde.inv%*%A.tilde
S.tilde.inv   = solve(S.tilde)

############################################################
# Normal-inverse Wishart posterior parameters
V.bar.inv   = t(X_p)%*%X_p + diag(1/diag(V.tilde))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X_p)%*%Y_p + diag(1/diag(V.tilde))%*%A.tilde)
nu.bar      = nrow(Y_p) + nu.tilde
S.bar       = S.tilde + t(Y_p)%*%Y_p + t(A.tilde)%*%diag(1/diag(V.tilde))%*%A.tilde -
              t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)
  

# Posterior draws 
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
L                 = t(chol(V.bar))
for (s in 1:S){
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
  }
sample_dummy_proof    = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)

# Report posterior means and sd of parameters
A.E         = round(apply(sample_dummy_proof$A.posterior,1:2,mean),6)
A.sd        = round(apply(sample_dummy_proof$A.posterior,1:2,sd),6)
Sigma.E     = round(apply(sample_dummy_proof$Sigma.posterior,1:2,mean),6)
Sigma.sd    = round(apply(sample_dummy_proof$Sigma.posterior,1:2,sd),6)

kable(A.E, format = "html", caption = "Posterior mean of autoregressive",
      col.names = paste0(1:ncol(A.E)))

kable(Sigma.E, format = "html", caption = "Posterior mean of covarince",
      col.names = paste0(1:ncol(Sigma.E)))

```




# References {.unnumbered}
Woźniak, T. (2016), Bayesian Vector Autoregressions. Australian Economic Review, 49: 365-380. https://doi.org/10.1111/1467-8462.12179

Giannone, Domenico, Lenza, Michele and Primiceri, Giorgio, (2015), Prior Selection for Vector Autoregressions, The Review of Economics and Statistics, 97, issue 2, p. 436-451.

Song, Y., & Woźniak, T.  (2021, March 25). Markov Switching. Oxford Research Encyclopedia of Economics and Finance. Retrieved 18 Jun. 2024, from https://oxfordre.com/economics/view/10.1093/acrefore/9780190625979.001.0001/acrefore-9780190625979-e-174.

Fu, W.; Smith, B.R.; Brewer, P.; Droms, S. Markov-Switching Bayesian Vector Autoregression Model in Mortality Forecasting. Risks 2023, 11, 152. https://doi.org/10.3390/risks11090152

