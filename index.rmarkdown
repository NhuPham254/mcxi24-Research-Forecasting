---
title: "Forecasting Unemployment rate using Bayesian VARs with Regime change"
author: "Nhu Minh Pham"

execute:
  echo: false
  
bibliography: references.bib
---


> **Abstract.** Research on how Bayesian VARs with regime change affect the forecasting 
of unemployment rate
>
> **Keywords.** bsvars, regime change, forecasting, R, 

# Introduction

The objective of this research project is to develop a Bayesian Vector Autoregression (BVAR) model incorporating regime changes to analyse the dynamic relationships among macroeconomic variables, assess their impact on the unemployment rate over different economic regimes through time-varying parameters, and enhance forecasting accuracy.

The question to be addressed in this research is: Can a Bayesian VAR model with regime changes accurately capture shifts in the relationships between unemployment rate and other macroeconomic variables for improved forecasting performance?

# Motivation
Understanding the impact of economic regime changes on the unemployment rate dynamics holds substantial importance in grasping the complexities of the labour market. The labour market is highly susceptible to various shocks and policy interventions, making accurate forecasting of the unemployment rate crucial for informed decision-making. By employing a Bayesian Vector Autoregression (BVAR) model that allows the parameters to vary across different economic regimes, we can gain a nuanced understanding of how the unemployment rate responds to diverse economic conditions and the efficacy of policies. This research enhances forecasting accuracy and offers valuable insights into the underlying drivers of unemployment fluctuations. By better understanding these dynamics, policymakers can make more informed decisions to counter economic instability.


# Data properties

For the investigation of the problem at hand, the selection of variables includes a comprehensive set of economic indicators and demographic factors that are crucial for understanding labour market dynamics and their impact on unemployment rates. 

For instance, GDP growth provides a broad measure of economic activity, serving as an indicator of overall labour market. Consumer price index, wage price index and interest rate reflect macroeconomic conditions and monetary policy, influencing consumer spending, business investment, and hiring decisions, thereby affecting unemployment trends. Government spending impacts aggregate demand and employment levels, while demographic factors such as population growth rate, average age of the workforce, and level of highest educational attainment offer insights into labour force participation and composition. 

Each variable's form/transformation will depend on its specific characteristics and the nature of its relationship with unemployment. For instance, variables like GDP growth may be included in their original form, while others, such as wpi might need to be computed from the index data. 

It's important to note that all data will be quarterly since the interest forecasting for unemployment will be conducted at a quarterly frequency. Quarterly frequency is suitable for capturing the diverse movements influencing unemployment, including short-term shocks, long-term trends, and policy changes, providing a balanced perspective.



\begin{align*}
& unmp_{t} & : & \text{Unemployment rate} \\
& par_{t} & : & \text{Participation rate} \\
& gdp_{t} & : & \text{GDP per capita: Chain volume measures - Percentage changes} \\
& cpi_{t} & : & \text{Consumer price index} \\
& wpi_{t} & : & \text{Wage price index} \\
& cashrate_{t} & : & \text{Cash rate} \\
& hrfull_{t} & : & \text{Average hours worked Full time} \\
& hrpart_{t} & : & \text{Average hours worked Part time} \\
& oversea_{t} & : & \text{Oversea migration} \\
& pop_{t} & : & \text{Population growth rate} \\
\end{align*}


```{r}
# Set global options to hide the source code, messages, and warnings
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```


# Data Visualisation

```{r, hide = TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library(readrba)
library(readabs)
library(ggplot2)
library(xts)
library(tseries)
library(MSwM)
```

```{r,echo=FALSE, warning=FALSE, message=FALSE}
start_date <- as.Date("1991-03-01")
end_date <- as.Date("2023-12-01")

########################################
# Define a function to process ABS data
########################################
process_abs_data <- function(series_id, start_date, end_date) {
  # Read raw data from ABS
  abs_data <- read_abs(series_id = series_id)
  
  # Convert to xts object
  abs_xts <- xts(abs_data$value, order.by = as.Date(abs_data$date))
  
  # Check if data is already in quarterly frequency
  if (abs_data$frequency == "Quarterly") {
    # Data is already in quarterly frequency
    abs_quarterly <- abs_xts
  } else {
    # Aggregate to quarterly frequency
    abs_quarterly <- apply.quarterly(abs_xts, mean)
  }
  
  # Convert to dataframe
  abs_df <- data.frame(Date = index(abs_quarterly), Value = coredata(abs_quarterly))
  
  # Subset to specified date range
  abs_df <- abs_df[abs_df$Date >= start_date & abs_df$Date <= end_date, ]
  
  # Return the processed dataframe
  return(abs_df)
}

########################################
# Define a function to process RBA data
########################################
process_rba_data <- function(series_id, start_date, end_date) {
  # Read raw data from ABS
  rba_data <- read_rba(series_id = series_id)
  
  # Convert to xts object
  rba_xts <- xts(rba_data$value, order.by = as.Date(rba_data$date))
  
  # Check if data is already in quarterly frequency
  if (rba_data$frequency == "Quarterly") {
    # Data is already in quarterly frequency
    rba_quarterly <- rba_xts
  } else {
    # Aggregate to quarterly frequency
    rba_quarterly <- apply.quarterly(rba_xts, mean)
  }
  
  # Convert to dataframe
  rba_df <- data.frame(Date = index(rba_quarterly), Value = coredata(rba_quarterly))
  
  # Subset to specified date range
  rba_df <- rba_df[rba_df$Date >= start_date & rba_df$Date <= end_date, ]
  
  # Return the processed dataframe
  return(rba_df)
}

```

```{r,echo=FALSE, warning=FALSE, message=FALSE}

#### check flow and stock, stock = end of quarter, flow= sum up

# Unemployment rate ################## "6202.0", A84423130A trend, A84423046K seasonadj
unmp_df <- process_abs_data("A84423046K", start_date, end_date)

# Participation rate ################## "6202.0", A84423051C seasonadj
par_df <- process_abs_data("A84423051C", start_date, end_date)

# GDP per capita: Chain volume measures; ##### "5206.0", A2304404C seasonadj, old A2304372W
gdp_df <- process_abs_data("A2304404C", start_date, end_date)

# WPI, Total hourly rates of pay including bonuses ###### 6345.0, A2713849C seasonadj
wpi_df <- process_abs_data("A2713849C", start_date, end_date)

# CPI ################## 6401.0, A3604506F seasonadj
cpi_df <- process_abs_data("A3604506F", start_date, end_date)

# Cash rate ################## original
cashrate_df <- process_rba_data("FIRMMCRT", start_date, end_date)

# Monthly hours works - Full time. "6202.0", A84426278A seasonadj
hrs_full_df <- process_abs_data("A84426278A", start_date, end_date)

# Monthly hours works - Part time. "6202.0", A84426279C seasonadj
hrs_part_df <- process_abs_data("A84426279C", start_date, end_date)

# Net Overseas Migration ################## "3101.0",  A2133254C original
oversea_df <- process_abs_data("A2133254C", start_date, end_date)

# Population ############# "3101.0", A2060842F original
pop_df <- process_abs_data("A2060842F", start_date, end_date)
```

```{r, plot, echo=FALSE, fig.width=8, fig.height=6}
dataframes <- list(unmp_df = "Unemployment rate",
                    par_df = "Participation rate",
                    gdp_df = "GDP per capita",
                    wpi_df = "WPI",
                    cpi_df = "CPI",
                    cashrate_df = "Cash rate",
                    hrs_full_df = "Hours worked Full time",
                    hrs_part_df = "Hours worked Part time",
                    oversea_df = "Net Overseas Migration",
                    pop_df = "Population growth")

par(mfrow = c(3, 4))

for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  plot(df$Date, y_values, type = "l", 
       xlab = "Date", 
       ylab = "Value", 
       main = dataframes[[df_name]])
}



```

<div id="figure1" style="text-align: center; color: #696969;">
###### Figure 1: Time series plots
</div>


```{r}
# Log-transformation
log_dataframes <- c("gdp_df", "wpi_df", "cpi_df", "hrs_full_df", "hrs_part_df", "oversea_df", "pop_df")

# Log-transform the specified dataframes
for (df_name in log_dataframes) {
  df <- get(df_name)
  df$Value <- log(df$Value)  # Log-transform the 'Value' column
  assign(df_name, df)         # Assign the modified dataframe back to its original name
}

oversea_df <- na.omit(oversea_df)

```

```{r, echo=FALSE, fig.width=8, fig.height=6}

dataframes <- list(unmp_df = "Unemployment rate",
                    par_df = "Participation rate",
                    gdp_df = "GDP per capita",
                    wpi_df = "WPI",
                    cpi_df = "CPI",
                    cashrate_df = "Cash rate",
                    hrs_full_df = "Hours worked Full time",
                    hrs_part_df = "Hours worked Part time",
                    oversea_df = "Net Overseas Migration",
                    pop_df = "Population growth")

par(mfrow = c(3, 4))

for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  plot(df$Date, y_values, type = "l", 
       xlab = "Date", 
       ylab = "Value", 
       main = dataframes[[df_name]])
}
```

To stabilize the variance, log tranformation is performed on all variables except 
unemployment rate, participation rate and cash rate.

From the plot of the variables, some show stationary and some non-stationary, which can make it challenging to analyse data. Therefore, the ACF test is performed to identify patterns and trend.



```{r,echo=FALSE, fig.width=8, fig.height=6}
par(mfrow = c(3, 4))

# Loop through each dataframe and create ACF plot
for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  acf(y_values, plot = TRUE, na.action = na.pass, main = paste("ACF of",
                                                               dataframes[[df_name]]))
}
```

<div id="figure2" style="text-align: center; color: #696969;">
###### Figure 2: ACF plots
</div>

The ACF plots show the variables have persistence or dependence in the data as the they are slowly decaying, indicates that there is a strong correlation between each variable 
and its past values, though this auto correlations decrease as lags increase.


```{r,echo=FALSE, fig.width=8, fig.height=6}
par(mfrow = c(3, 4))

# Loop through each dataframe and create ACF plot
for (df_name in names(dataframes)) {
  df <- get(df_name)
  y_values <- df$Value
  pacf(y_values, plot = TRUE, na.action = na.pass, main = paste("PACF of",
                                                               dataframes[[df_name]]))
}
```

<div id="figure3" style="text-align: center; color: #696969;">
###### Figure 3: PACF plots
</div>

The PACF plots show high autocorrelation at lag 1, and have clear cut offs.

The ADF is performed to further determine the stationary properties of the data.
Null hypothesis: a unit root is present
Alternative hypothesis: stationary

```{r}
# Perform ADF test for each variable
dataframes <- list(unmp_df = "Unemployment rate",
                    par_df = "Participation rate",
                    gdp_df = "GDP per capita",
                    wpi_df = "WPI",
                    cpi_df = "CPI",
                    cashrate_df = "Cash rate",
                    hrs_full_df = "Hours worked Full time",
                    hrs_part_df = "Hours worked Part time",
                    oversea_df = "Net Overseas Migration",
                    pop_df = "Population growth")

# Create an empty dataframe to store ADF test results
adf <- data.frame(Dickey_Fuller = numeric(length(dataframes)), 
                  p_value = numeric(length(dataframes)))

# Set row names to variable names
rownames(adf) <- names(dataframes)

# Perform ADF test for each variable
for (i in 1:length(dataframes)) {
  df <- get(names(dataframes)[i])  # Get the dataframe corresponding to the current variable
  adf_tmp <- adf.test(df$Value)    # Perform ADF test on the 'Value' column of the dataframe
  adf[i, "Dickey_Fuller"] <- as.numeric(adf_tmp$statistic)
  adf[i, "p_value"] <- as.numeric(adf_tmp$p.value)
}

# Round the numeric values to desired precision
adf <- round(adf, 3)

# Print the ADF test results
print(adf)
```



## Model

### Hypothesis

Var(p) model
\begin{aligned}
y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 x_{1, t-1} + \beta_3 x_{2, t-1} + \ldots + \beta_n x_{n, t-1} + \varepsilon_t
\end{aligned}


Var(p) model with regime change, meaning time-varying parameters
\begin{aligned}
y_t &= \beta_{0,S_t} + \beta_{1,S_t} y_{t-1} + \beta_{2,S_t} x_{1, t-1} + \beta_{3,S_t} x_{2, t-1} + \ldots + \beta_{n,S_t} x_{n, t-1} + \varepsilon_t \\
\end{aligned}

Probability of transitioning from one state to another
\begin{aligned}
P(S_t = j | S_{t-1} = i) &= \pi_{ij}
\end{aligned}



### Matrix form


\begin{align*}
\mathbf{Y}_t = \boldsymbol{\beta}_{S_t} \mathbf{X}_t + \boldsymbol{E}_t
\end{align*}

\begin{align*}
\boldsymbol{E}_t
\sim MN(0_T, \Sigma, I_T)
\end{align*}

\\begin{align*}
\\mathbf{Y}_t & : \\text{Matrix of response variables} \\\\
\\boldsymbol{\\beta} & : \\text{Coefficient matrix corresponding to regime state} \\\\
S_t & : \\text{Regime state} \\\\
\\mathbf{X}_t & : \\text{Matrix of predictor variables} \\\\
\\boldsymbol{\\varepsilon}_t & : \\text{Error term vector} 
\\end{align*}

\\begin{align*}
\\mathbf{Y}_t = \\begin{pmatrix}
\\text{Unemployment}_t \\\\
\\text{Participation}_t \\\\
\\text{GDP}_t \\\\
\\text{WPI}_t \\\\
\\text{CPI}_t \\\\
\\text{CashRate}_t \\\\
\\text{HoursFull}_t \\\\
\\text{HoursPart}_t \\\\
\\text{OverseasMigration}_t \\\\
\\text{Population}_t \\\\
\\end{pmatrix}
\\end{align*}

The model's equations include time-varying parameters, such as the coefficients of lagged variables, which adapt to changing economic conditions.


```{r}
# Combine data frames into one data frame
# Truncate dates to the first day of the month
cashrate_df$Date <- as.Date(format(cashrate_df$Date, "%Y-%m-01"))
# Merge data frames by Date
merged_df <- merge(unmp_df, gdp_df, by = "Date", suffixes = c("_unmp", "_gdp"))
merged_df <- merge(merged_df, wpi_df, by = "Date", suffixes = c("", "_wpi"))
merged_df <- merge(merged_df, cashrate_df, by = "Date", suffixes = c("", "_cash"))

```


## Basic Model
The model follows the Normal Inverse Wishart distribution.

### Minnesota Prior distribution

The Minnesota prior is commonly used in Bayesian Vector Autoregression (BVAR) models due to its ability to impose shrinkage towards zero on the coefficients, effectively regularizing the estimation process. The Minnesota prior aligns effectively with the stylized fact of nonstationarity observed in macroeconomic variables.

\begin{align*}
p(A, \Sigma) = p(A \mid \Sigma) \cdot p(\Sigma) \\
A \mid \Sigma \sim \text{MN}_{K \times N} (\underline{A}, \Sigma, \underline{V}) \\
\Sigma \sim  \text{IW}_{N} (\underline{S}, \underline{v})\\
\end{align*}

With lags = 4 and N = 4

\begin{align*}
\underline{A} = \begin{bmatrix}
\mathbf{0}_{4 \times 1} & \mathbf{I}_{4} & \mathbf{0}_{4 \times (4-1)4}
\end{bmatrix}'
= \begin{bmatrix}
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\end{bmatrix}'
\end{align*}

The column-specific prior covariance of A
\begin{align*}
\underline{V} &= \text{diag}\left( \begin{bmatrix}
k_2 & k_1(\mathbf{p}^{-2} \otimes 1'_4)
\end{bmatrix} \right)\\
\mathbf{p} &= \begin{bmatrix}
1 & 2 & 3 & 4 \\
\end{bmatrix}
\end{align*}
\begin{align*}
& k_2 : \text{overall shrinkage for the constant term} \\
& k_1 : \text{overall shrinkage levels for autoregressive slopes} \\
\end{align*}

Prior covariance matrix
\begin{bmatrix}
k_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & k_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & k_1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & k_1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & k_1 & 0 & 0 & 0 & 0 & \cdots & 0\\ 
0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & 0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4} & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{16}      
\end{bmatrix}



```{r}
# Data configuration
N       = 4
p       = 4
S       = 1000

y       = merged_df
y       = ts(y[,c(2,3,4,5)], start=c(1997,9), frequency=4,
             names=c("unmp","gdp","wpi","cashrate"))
Y       = ts(y[5:nrow(y),], start=c(1998,9), frequency=4)

X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:nrow(y)-i,])
}
```


### The joint posterior distribution

\begin{align}
p(A,\Sigma | Y,X) &= p(A|Y,X,\Sigma)p(\Sigma|Y,X) \\
p(A|Y,X,\Sigma) &\sim MN_{K \times N}(\overline{A}, \Sigma,\overline{V} ) \\ 
p(\Sigma|Y,X) &\sim IW_{N}(\overline{S}, \overline{v})\\
\\
\overline{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1}\underline{A}) \\
\overline{v} &= T + \underline{v} \\ 
\overline{S} &= \underline{S}+Y'Y+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A}'
\overline{V}^{-1}\overline{A} \\
\end{align}



```{r, results='hide'}
# MLE
############################################################
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T
round(A.hat,3)
round(Sigma.hat,3)
round(cov2cor(Sigma.hat),3)

# Prior distribution
############################################################
kappa.1     = 0.02^2
kappa.2     = 100
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:5, 1:4] <- diag(1, nrow = 4)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

# Normal-inverse Wishard posterior parameters
############################################################
V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y) + nu.prior
S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)
```


1000 Sample draws from the joint distribution with Minnesota prior.

```{r}
# posterior draws 
############################################################
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
L                 = t(chol(V.bar))
for (s in 1:S){
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
}

# round(apply(A.posterior,1:2,mean),3)
```

```{r, results='hide' }
# report posterior means and sd of parameters
A.E         = apply(A.posterior,1:2,mean)
A.sd        = apply(A.posterior,1:2,sd)
Sigma.E     = apply(Sigma.posterior,1:2,mean)
Sigma.sd    = apply(Sigma.posterior,1:2,sd)

library(xtable)
xtable(rbind(format(t(A.E),digits=1, scientific=FALSE)[1,],
      format(t(A.sd),digits=1, scientific=FALSE)[1,],
      format(t(A.E),digits=1, scientific=FALSE)[2,],
      format(t(A.sd),digits=1, scientific=FALSE)[2,]))

xtable(rbind(format(Sigma.E,digits=1, scientific=FALSE)[1,],
             format(Sigma.sd,digits=1, scientific=FALSE)[1,],
             format(Sigma.E,digits=1, scientific=FALSE)[2,],
             format(Sigma.sd,digits=1, scientific=FALSE)[2,]))


# simulate draws from the predictive density
# WARNING! This takes a while
############################################################

```

Table 

\begin{table}[ht]
\centering
\begin{tabular}{rlllllllllllllllll}
  \hline
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 \\ 
  \hline
1 & 48.09440166 &  1.28178657 &  0.03163478 &  0.04085992 & -0.12287201 & -0.30906308 &  0.00141876 &  0.00525711 & -0.00867471 & -0.12901623 &  0.01479746 & -0.00816151 & -0.00361686 &  0.07787646 &  0.00557509 &  0.01145551 &  0.01212629 \\ 
  2 & 34.394276 &  0.118867 &  0.824480 &  0.836935 &  0.790465 &  0.171764 &  0.403564 &  0.418450 &  0.397149 &  0.145780 &  0.276347 &  0.268899 &  0.265922 &  0.099096 &  0.197875 &  0.207806 &  0.205609 \\ 
  3 & -0.00906363 &  0.00004789 &  1.00000271 & -0.00001920 &  0.00011165 &  0.00000768 & -0.00000076 & -0.00000147 &  0.00002436 & -0.00005963 & -0.00000272 &  0.00000277 &  0.00001155 &  0.00002312 &  0.00000123 &  0.00000093 &  0.00000380 \\ 
  4 &  0.011129 &  0.000041 &  0.000297 &  0.000284 &  0.000281 &  0.000061 &  0.000144 &  0.000143 &  0.000143 &  0.000053 &  0.000098 &  0.000101 &  0.000096 &  0.000035 &  0.000072 &  0.000071 &  0.000071 \\ 
   \hline
\end{tabular}
\end{table}


Y.h         = array(NA,c(h,N,S))

for (s in 1:S){
  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]
  x.Ti        = x.Ti[4:1,]
  for (i in 1:h){
    x.T         = c(1,as.vector(t(x.Ti)))
    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%A.posterior[,,s], sigma=Sigma.posterior[,,s])
    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:3,])
  }
}



#######################################################################################
## Extension 1 Model
Cointegration is when the non-stationary variables have an equilibrium long term 
relationship or a common trend. The model is extended to incorporate dummy-observation-prior, aiming to account for the potential cointegration amongst the variables, capturing the underlying economic relationships and dynamics.

This approach allows us to leverage both the observed data and additional information about the structural relationships among variables, leading to more accurate parameter estimation and potentially improved forecasting performance.

### Minnesota and Dummy Observation Prior distribution

\begin{align*}
p(A, \Sigma) = p(A \mid \Sigma) \cdot p(\Sigma) \\
A \mid \Sigma \sim \text{MN}_{K \times N} (\underline{A}, \Sigma, \underline{V}, \lambda) \\
\Sigma \sim  \text{IW}_{N} (\underline{S}, \underline{v})\\
\end{align*}

With lags = 4 and N = 4

Prior covariance matrix
\begin{bmatrix}
k_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & \frac{k_1}{\lambda} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & \frac{k_1}{\lambda} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \frac{k_1}{\lambda} & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \frac{k_1}{\lambda} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & \frac{k_1}{4\lambda} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4\lambda} & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4\lambda} & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{k_1}{4\lambda} \\
\end{bmatrix}


# MLE
############################################################
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T
round(A.hat,3)
round(Sigma.hat,3)
round(cov2cor(Sigma.hat),3)

# Prior distribution
############################################################
kappa.1     = 10
kappa.2     = 100
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:4,] <- diag(1)  ??????
V.prior     = diag(c(kappa.2,(1/lambda)*kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1
```
```




```{r}
# Need Model
# Need number of regime states -> AIC,BIC? No need, r=2
# dummy obse prior+minnesota, 2021, 2016

# state 2
# Need lag
# To estimate S, How do i want to model St
# Mackov switching or assume S



# Find number of regime states
# mod.mswm <- msmFit(mod,k=2,p=0,sw=c(TRUE,TRUE),control=list(parallel=TRUE))

```




# References {.unnumbered}
